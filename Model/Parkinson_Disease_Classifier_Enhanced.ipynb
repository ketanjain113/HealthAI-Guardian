{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, numpy as np, h5py, pickle\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "TARGET_SIZE = (128, 128)\n",
    "MODEL_H5_PATH = 'parkinson_model.h5'\n",
    "DATA_DIR = 'Parkinson-s-Disease-Classifier/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: return None\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    return cv2.resize(clahe.apply(img), TARGET_SIZE)\n",
    "\n",
    "def extract_features(img):\n",
    "    f = [np.mean(img), np.std(img), np.median(img), np.min(img), np.max(img),\n",
    "         np.percentile(img,10), np.percentile(img,25), np.percentile(img,75), np.percentile(img,90), np.var(img)]\n",
    "    hist, _ = np.histogram(img.flatten(), bins=16, range=(0,256))\n",
    "    f.extend((hist/(hist.sum()+1e-7)).tolist())\n",
    "    try:\n",
    "        glcm = graycomatrix(img, [1], [0], 256, True, True)\n",
    "        for p in ['contrast','dissimilarity','homogeneity','energy','correlation','ASM']: f.append(graycoprops(glcm,p)[0,0])\n",
    "    except: f.extend([0]*6)\n",
    "    edges = cv2.Canny(img, 50, 150)\n",
    "    f.extend([np.mean(edges), np.std(edges), np.sum(edges>0)/edges.size, np.max(edges)])\n",
    "    sx, sy = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=3), cv2.Sobel(img,cv2.CV_64F,0,1,ksize=3)\n",
    "    f.extend([np.mean(np.abs(sx)), np.mean(np.abs(sy)), np.std(sx), np.std(sy)])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    imgs, lbls = [], []\n",
    "    for s in ['train','test']:\n",
    "        for f in glob(f'{DATA_DIR}/{s}/parkinson/*.*'):\n",
    "            i = preprocess_image(f)\n",
    "            if i is not None: imgs.append(i); lbls.append(1)\n",
    "        for f in glob(f'{DATA_DIR}/{s}/healthy/*.*'):\n",
    "            i = preprocess_image(f)\n",
    "            if i is not None: imgs.append(i); lbls.append(0)\n",
    "    print(f'Loaded: {sum(lbls)} PD, {len(lbls)-sum(lbls)} Healthy')\n",
    "    return imgs, lbls\n",
    "\n",
    "def augment(imgs, lbls):\n",
    "    aug_i, aug_l = list(imgs), list(lbls)\n",
    "    for i, l in zip(imgs, lbls):\n",
    "        aug_i.append(cv2.flip(i,1)); aug_l.append(l)\n",
    "        for a in [5,-5]:\n",
    "            M = cv2.getRotationMatrix2D((64,64), a, 1.0)\n",
    "            aug_i.append(cv2.warpAffine(i, M, TARGET_SIZE)); aug_l.append(l)\n",
    "    print(f'Augmented: {len(imgs)} -> {len(aug_i)}')\n",
    "    return aug_i, aug_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, lbls = load_data()\n",
    "imgs, lbls = augment(imgs, lbls)\n",
    "print('Extracting features...')\n",
    "X = np.array([extract_features(i) for i in imgs])\n",
    "y = np.array(lbls)\n",
    "print(f'Features: {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_tr_s = scaler.fit_transform(X_tr)\n",
    "X_te_s = scaler.transform(X_te)\n",
    "print(f'Train: {len(X_tr)} | Test: {len(X_te)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Ensemble...')\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_split=3, random_state=42, n_jobs=-1)\n",
    "xgb = XGBClassifier(n_estimators=300, max_depth=8, learning_rate=0.1, subsample=0.8, random_state=42, n_jobs=-1)\n",
    "gb = GradientBoostingClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42)\n",
    "model = VotingClassifier(estimators=[('rf',rf),('xgb',xgb),('gb',gb)], voting='soft', n_jobs=-1)\n",
    "model.fit(X_tr_s, y_tr)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_te_s)\n",
    "acc = accuracy_score(y_te, y_pred)\n",
    "print(f'Accuracy: {acc:.2%}')\n",
    "print(classification_report(y_te, y_pred, target_names=['Healthy','Parkinson']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(MODEL_H5_PATH, 'w') as h5f:\n",
    "    h5f.attrs['model_name'] = 'Enhanced_Ensemble'\n",
    "    h5f.attrs['accuracy'] = float(acc)\n",
    "    sg = h5f.create_group('scaler')\n",
    "    sg.create_dataset('mean', data=scaler.mean_)\n",
    "    sg.create_dataset('scale', data=scaler.scale_)\n",
    "    pg = h5f.create_group('pickle_model')\n",
    "    pg.create_dataset('data', data=np.frombuffer(pickle.dumps({'model':model,'scaler':scaler}), dtype=np.uint8))\n",
    "print(f'Model saved to {MODEL_H5_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(path):\n",
    "    with h5py.File(path,'r') as h: return pickle.loads(bytes(h['pickle_model']['data'][:]))\n",
    "\n",
    "data = load_h5(MODEL_H5_PATH)\n",
    "m, s = data['model'], data['scaler']\n",
    "print(f'Loaded accuracy: {accuracy_score(y_te, m.predict(X_te_s)):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path, model, scaler):\n",
    "    img = preprocess_image(path)\n",
    "    if img is None: return None\n",
    "    f = np.array(extract_features(img)).reshape(1,-1)\n",
    "    p = model.predict(scaler.transform(f))[0]\n",
    "    prob = model.predict_proba(scaler.transform(f))[0]\n",
    "    return {'pred': 'Parkinson' if p==1 else 'Healthy', 'conf': prob[p]}\n",
    "\n",
    "print('Sample predictions:')\n",
    "for d in [f'{DATA_DIR}/test/parkinson', f'{DATA_DIR}/test/healthy']:\n",
    "    for p in glob(f'{d}/*.*')[:2]:\n",
    "        r = predict(p, m, s)\n",
    "        if r: print(f'  {os.path.basename(p)[:25]} -> {r[\"pred\"]} ({r[\"conf\"]:.1%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Accuracy: 92.68%\n",
    "- Model: Ensemble (RF+XGB+GB)\n",
    "- Output: parkinson_model.h5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
